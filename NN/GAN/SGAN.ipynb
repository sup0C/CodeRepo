{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow.keras.datasets.mnist as mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "gpu = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "(x_train,_),(x_test,test_lable)=tf.keras.datasets.mnist.load_data()\n",
    "print(x_train.shape,y_train.shape,x_train.dtype)\n",
    "\n",
    "# 转换数据类型\n",
    "x_train=((x_train.astype('float32')-127.5)/127.5).reshape(x_train.shape[0],28,28,1)\n",
    "print(x_train.shape)\n",
    "\n",
    "#  定义datasets\n",
    "batch_szie=256\n",
    "shuffle_size=x_train.shape[0]\n",
    "noise_dim=100 # 输入G中的初始向量长度。\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(train_image)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((test_image, test_lable))\n",
    "dataset_train = dataset_train.shuffle(shuffle_size).batch(BATCH_SIZE)\n",
    "dataset_test = dataset_test.repeat().shuffle(shuffle_size).batch(BATCH_SIZE)\n",
    "\n",
    "dataset_train = datasets.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "dataset_test = datasets.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "# Dataset.prefetch() 方法，使得我们可以让数据集对象 Dataset 在训练时预取出若干个元素，\n",
    "# 使得在 GPU 训练的同时 CPU 可以准备数据，从而提升训练流程的效率\n",
    "# 使用方法和Dataset.batch() 、 Dataset.shuffle() 等非常类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    seed = layers.Input(shape=((noise_dim,)))\n",
    "    \n",
    "    x = layers.Dense(3*3*256, use_bias=False)(seed)\n",
    "    x = layers.Reshape((3, 3, 256))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)     #  7*7\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)    #   14*14\n",
    "\n",
    "    x = layers.Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='same')(x)  # 28*28*1\n",
    "    \n",
    "    x = tf.tanh(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=seed, outputs=x)  \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    \n",
    "    image = tf.keras.Input(shape=((28, 28, 1)))\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same')(image)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "      \n",
    "    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "#    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = layers.Dense(11, activation='softmax')(x)  # 一共十类，输出11类。第11类为生成图像的类别\n",
    "    \n",
    "    model = tf.keras.Model(inputs=image, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "generator = generator_model()\n",
    "discriminator = discriminator_model()\n",
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(d_label_real_output, label, d_unlabel_real_output, d_fake_output):\n",
    "    label_real_loss = cross_entropy(label, d_label_real_output[:, :-1])\n",
    "    \n",
    "    unlabel_real_loss = binary_cross_entropy(tf.zeros_like(d_unlabel_real_output[:, -1]), \n",
    "                                             d_unlabel_real_output[:, -1])\n",
    "    \n",
    "    fake_loss = binary_cross_entropy(tf.ones_like(d_fake_output[:, -1]), d_fake_output[:, -1])\n",
    "    \n",
    "    return  label_real_loss + unlabel_real_loss + fake_loss\n",
    "\n",
    "def generator_loss(d_fake_output):\n",
    "#    fake_loss = -tf.reduce_mean(tf.math.log(d_fake_output[:, -1] + 1e-07))\n",
    "#    input_real = tf.cast(input_real, tf.float32)\n",
    "    fake_loss = binary_cross_entropy(tf.zeros_like(d_fake_output[:, -1]), d_fake_output[:, -1])\n",
    "#    l2_loss = tf.reduce_mean(tf.math.square(input_real - g_output))\n",
    "    return fake_loss\n",
    "\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_label_real, labels, input_unlabel_real):\n",
    "    '''\n",
    "    input_label_real: 为x_test\n",
    "    labels: 为y_test\n",
    "    input_unlabel_real:为x_train\n",
    "    '''\n",
    "    \n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        \n",
    "        generated_images = generator(noise, training=True)\n",
    "        d_fake_output = discriminator(generated_images, training=True) \n",
    "        \n",
    "        d_label_real_output = discriminator(input_label_real, training=True)\n",
    "        \n",
    "        d_unlabel_real_output = discriminator(input_unlabel_real, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(d_fake_output)\n",
    "        disc_loss = discriminator_loss(d_label_real_output, labels, d_unlabel_real_output, d_fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "noise_dim = 100\n",
    "num = 10\n",
    "noise_seed = tf.random.normal([num, noise_dim])\n",
    "\n",
    "def generate_and_save_images(model, noise_input,  epoch):\n",
    "    print('Epoch:', epoch+1)\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(noise_input, training=False)\n",
    "    predictions = np.squeeze(predictions)\n",
    "    fig = plt.figure(figsize=(10, 1))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(1, 10, i+1)\n",
    "        plt.imshow((predictions[i, :, :] + 1)/2, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "#    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "epoch_loss_avg_gen = tf.keras.metrics.Mean('g_loss')\n",
    "epoch_loss_avg_disc = tf.keras.metrics.Mean('d_loss')\n",
    "\n",
    "g_loss_results = []\n",
    "d_loss_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f41d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_with_label, dataset_no_label, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for (label_image_batch, label_batch), unlabel_image_batch in zip(dataset_with_label, dataset_no_label):  # test有label，train无label\n",
    "            g_loss, d_loss = train_step(label_image_batch, label_batch, unlabel_image_batch)\n",
    "            epoch_loss_avg_gen(g_loss)\n",
    "            epoch_loss_avg_disc(d_loss)\n",
    "        print()\n",
    "        g_loss_results.append(epoch_loss_avg_gen.result())\n",
    "        d_loss_results.append(epoch_loss_avg_disc.result())\n",
    "        \n",
    "        epoch_loss_avg_gen.reset_states()\n",
    "        epoch_loss_avg_disc.reset_states()\n",
    "        \n",
    "        if epoch%20 == 0:\n",
    "            generate_and_save_images(generator,\n",
    "                                     noise_seed,\n",
    "                                     epoch)\n",
    "\n",
    "    generate_and_save_images(generator,\n",
    "                            noise_seed,\n",
    "                            epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb20fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "train(dataset_test, dataset_train, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e948000",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(g_loss_results)+1), g_loss_results, label='g_loss')\n",
    "plt.plot(range(1, len(d_loss_results)+1), d_loss_results, label='d_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30034098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "generator.save('gen_model/generate_SGAN.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycharm_venv",
   "language": "python",
   "name": "pycharm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
