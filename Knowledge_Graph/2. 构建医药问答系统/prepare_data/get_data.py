import urllib.request
import urllib.parse
from lxml import etree
import json
import time
import os

'''
ç–¾ç—…ä¿¡æ¯çˆ¬è™«ï¼ˆæ— æ•°æ®åº“ç‰ˆï¼‰
åŠŸèƒ½ï¼šé‡‡é›†ç–¾ç—…è¯¦æƒ…å¹¶ä¿å­˜ä¸º JSON æ–‡ä»¶
'''


class DiseaseSpider:
    def __init__(self,disease_file='diseases.json',inspect_file='inspects.json'):
        '''
        disease_file:ç–¾ç—…ç›¸å…³æè¿°çš„jsonæ–‡ä»¶ä¿å­˜è·¯å¾„
        inspect_file:åŒ»å­¦æ£€æŸ¥é¡¹ç›¸å…³æè¿°çš„jsonæ–‡ä»¶ä¿å­˜è·¯å¾„
        '''
        # ä¿å­˜æ–‡ä»¶è·¯å¾„
        self.disease_file =disease_file
        self.inspect_file =inspect_file

    '''æ ¹æ® URL è·å– HTML å†…å®¹'''

    def get_html(self, url):
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 '
                          '(KHTML, like Gecko) Chrome/51.0.2704.63 Safari/537.36'
        }
        req = urllib.request.Request(url=url, headers=headers)
        try:
            res = urllib.request.urlopen(req, timeout=10)
            html = res.read()
            # å°è¯•ç”¨ utf-8 è§£ç ï¼Œå¤±è´¥åˆ™ç”¨ gbk
            try:
                return html.decode('utf-8')
            except UnicodeDecodeError:
                return html.decode('gbk')
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥: {url}, é”™è¯¯: {e}")
            return ''

    '''ä¸»å‡½æ•°ï¼šçˆ¬å–ç–¾ç—…æ•°æ®å¹¶ä¿å­˜ä¸º JSON'''

    def spider_main(self, start_page=1, end_page=100):
        print(f"å¼€å§‹çˆ¬å–ç–¾ç—…æ•°æ®ï¼ˆ{start_page} ~ {end_page}ï¼‰...")
        all_data = []

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½å·²æœ‰æ•°æ®ï¼ˆå®ç°å¢é‡é‡‡é›†ï¼‰
        if os.path.exists(self.disease_file):
            with open(self.disease_file, 'r', encoding='utf-8') as f:
                all_data = json.load(f)
            print(f"å·²åŠ è½½ {len(all_data)} æ¡å†å²æ•°æ®")

        existing_ids = {item['page_id'] for item in all_data}  # é¿å…é‡å¤é‡‡é›†

        for page in range(start_page, end_page + 1):
            if page in existing_ids:
                print(f"[è·³è¿‡] ç¬¬ {page} é¡µå·²å­˜åœ¨")
                continue

            try:
                basic_url = f'http://jib.xywy.com/il_sii/gaishu/{page}.htm'
                cause_url = f'http://jib.xywy.com/il_sii/cause/{page}.htm'
                prevent_url = f'http://jib.xywy.com/il_sii/prevent/{page}.htm'
                symptom_url = f'http://jib.xywy.com/il_sii/symptom/{page}.htm'
                inspect_url = f'http://jib.xywy.com/il_sii/inspect/{page}.htm'
                treat_url = f'http://jib.xywy.com/il_sii/treat/{page}.htm'
                food_url = f'http://jib.xywy.com/il_sii/food/{page}.htm'
                drug_url = f'http://jib.xywy.com/il_sii/drug/{page}.htm'

                data = {
                    'page_id': page,
                    'url': basic_url,
                    'basic_info': self.basicinfo_spider(basic_url),
                    'cause_info': self.common_spider(cause_url),
                    'prevent_info': self.common_spider(prevent_url),
                    'symptom_info': self.symptom_spider(symptom_url),
                    'inspect_info': self.inspect_spider(inspect_url),
                    'treat_info': self.treat_spider(treat_url),
                    'food_info': self.food_spider(food_url),
                    'drug_info': self.drug_spider(drug_url)
                }

                # åªæœ‰æˆåŠŸé‡‡é›†æ‰æ·»åŠ 
                if data['basic_info']:  # åŸºæœ¬ä¿¡æ¯å­˜åœ¨è¯´æ˜é¡µé¢æœ‰æ•ˆ
                    all_data.append(data)
                    print(f"[æˆåŠŸ] çˆ¬å–ç¬¬ {page} é¡µ: {data['basic_info']['name']}")
                else:
                    print(f"[ç©ºé¡µ] ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œè·³è¿‡")

            except Exception as e:
                print(f"[å¤±è´¥] çˆ¬å–ç¬¬ {page} é¡µå¤±è´¥: {e}")

            # é˜²å°ï¼šæ¯çˆ¬ä¸€é¡µæš‚åœ 1 ç§’
            time.sleep(1)

        # ä¿å­˜åˆ° JSON æ–‡ä»¶
        with open(self.disease_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç–¾ç—…æ•°æ®å·²ä¿å­˜åˆ° {self.disease_file}ï¼Œå…± {len(all_data)} æ¡")

    '''åŸºæœ¬ä¿¡æ¯è§£æ'''

    def basicinfo_spider(self, url):
        html = self.get_html(url)
        if not html:
            return {}
        selector = etree.HTML(html)
        try:
            title = selector.xpath('//title/text()')[0]
            category = selector.xpath('//div[@class="wrap mt10 nav-bar"]/a/text()')
            desc_list = selector.xpath('//div[@class="jib-articl-con jib-lh-articl"]/p/text()')
            ps = selector.xpath('//div[@class="mt20 articl-know"]/p')
            infobox = []
            for p in ps:
                info = p.xpath('string(.)').replace('\r', '').replace('\n', '').replace('\xa0', '').replace('   ',
                                                                                                            '').replace(
                    '\t', '')
                if info.strip():
                    infobox.append(info.strip())

            return {
                'category': [cat.strip() for cat in category],
                'name': title.replace('çš„ç®€ä»‹', '').strip(),
                'desc': ''.join(desc_list).strip(),
                'attributes': infobox
            }
        except Exception as e:
            print(f"è§£æ basic_info å¤±è´¥ {url}: {e}")
            return {}

    '''æ²»ç–—ä¿¡æ¯è§£æ'''

    def treat_spider(self, url):
        html = self.get_html(url)
        if not html:
            return []
        selector = etree.HTML(html)
        ps = selector.xpath('//div[starts-with(@class,"mt20 articl-know")]/p')
        infobox = []
        for p in ps:
            info = p.xpath('string(.)').replace('\r', '').replace('\n', '').replace('\xa0', '').replace('   ',
                                                                                                        '').replace(
                '\t', '')
            if info.strip():
                infobox.append(info.strip())
        return infobox

    '''è¯å“æ¨èè§£æ'''

    def drug_spider(self, url):
        html = self.get_html(url)
        if not html:
            return []
        selector = etree.HTML(html)
        drugs = [name.replace('\n', '').replace('\t', '').replace(' ', '')
                 for name in selector.xpath('//div[@class="fl drug-pic-rec mr30"]/p/a/text()')]
        return drugs

    '''é¥®é£Ÿå»ºè®®è§£æ'''

    def food_spider(self, url):
        html = self.get_html(url)
        if not html:
            return {}
        selector = etree.HTML(html)
        divs = selector.xpath('//div[@class="diet-img clearfix mt20"]')
        try:
            good = [food.strip() for food in divs[0].xpath('./div/p/text()')]
            bad = [food.strip() for food in divs[1].xpath('./div/p/text()')]
            recommand = [food.strip() for food in divs[2].xpath('./div/p/text()')]
            return {'good': good, 'bad': bad, 'recommand': recommand}
        except:
            return {}

    '''ç—‡çŠ¶ä¿¡æ¯è§£æ'''

    def symptom_spider(self, url):
        html = self.get_html(url)
        if not html:
            return {'symptoms': [], 'symptoms_detail': []}
        selector = etree.HTML(html)
        symptoms = selector.xpath('//a[@class="gre"]/text()')
        ps = selector.xpath('//p')
        detail = []
        for p in ps:
            info = p.xpath('string(.)').replace('\r', '').replace('\n', '').replace('\xa0', '').replace('   ',
                                                                                                        '').replace(
                '\t', '')
            if info.strip():
                detail.append(info.strip())
        return {'symptoms': symptoms, 'symptoms_detail': detail}

    '''æ£€æŸ¥é¡¹ç›®è§£æ'''

    def inspect_spider(self, url):
        html = self.get_html(url)
        if not html:
            return []
        selector = etree.HTML(html)
        inspects = selector.xpath('//li[@class="check-item"]/a/@href')
        return inspects

    '''é€šç”¨æ–‡æœ¬è§£ææ¨¡å—'''

    def common_spider(self, url):
        html = self.get_html(url)
        if not html:
            return ''
        selector = etree.HTML(html)
        ps = selector.xpath('//p')
        texts = []
        for p in ps:
            info = p.xpath('string(.)').replace('\r', '').replace('\n', '').replace('\xa0', '').replace('   ',
                                                                                                        '').replace(
                '\t', '')
            if info.strip():
                texts.append(info.strip())
        return '\n'.join(texts)

    '''æ£€æŸ¥é¡¹é¡µé¢ HTML æŠ“å–ï¼ˆä¿å­˜ä¸º JSONï¼‰'''

    def inspect_crawl(self, start_page=1, end_page=3684):
        print(f"å¼€å§‹çˆ¬å–æ£€æŸ¥é¡¹é¡µé¢ HTMLï¼ˆ{start_page} ~ {end_page}ï¼‰...")
        all_inspects = []

        if os.path.exists(self.inspect_file):
            with open(self.inspect_file, 'r', encoding='utf-8') as f:
                all_inspects = json.load(f)
            print(f"å·²åŠ è½½ {len(all_inspects)} æ¡æ£€æŸ¥é¡¹æ•°æ®")

        existing_ids = {item['page_id'] for item in all_inspects}

        for page in range(start_page, end_page + 1):
            if page in existing_ids:
                print(f"[è·³è¿‡] æ£€æŸ¥é¡¹ç¬¬ {page} é¡µå·²å­˜åœ¨")
                continue

            try:
                url = f'http://jck.xywy.com/jc_{page}.html'
                html = self.get_html(url)
                if html:
                    data = {
                        'page_id': page,
                        'url': url,
                        'html': html  # å¯æ”¹ä¸ºåªå­˜å…³é”®éƒ¨åˆ†ä»¥èŠ‚çœç©ºé—´
                    }
                    all_inspects.append(data)
                    print(f"[æˆåŠŸ] çˆ¬å–æ£€æŸ¥é¡¹ç¬¬ {page} é¡µ")
                else:
                    print(f"[å¤±è´¥] è·å–æ£€æŸ¥é¡¹ç¬¬ {page} é¡µå¤±è´¥")
            except Exception as e:
                print(f"[å¼‚å¸¸] æ£€æŸ¥é¡¹ç¬¬ {page} é¡µ: {e}")

            time.sleep(1)  # é˜²å°

        # ä¿å­˜
        with open(self.inspect_file, 'w', encoding='utf-8') as f:
            json.dump(all_inspects, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ£€æŸ¥é¡¹æ•°æ®å·²ä¿å­˜åˆ° {self.inspect_file}")


# ========================
#  ğŸš€ è¿è¡Œçˆ¬è™«
# ========================
if __name__ == '__main__':
    spider = DiseaseSpider(disease_file='diseases.json',
                           inspect_file='inspects.json')
    # é€‰æ‹©è¿è¡Œä¸€ä¸ªä»»åŠ¡ï¼š
    # 1. çˆ¬ç–¾ç—…æ•°æ®ï¼ˆå»ºè®®å…ˆè¯• 1-10ï¼‰
    spider.spider_main(start_page=1, end_page=10) #  end_page=11000

    # 2. çˆ¬åŒ»å­¦æ£€æŸ¥é¡¹ç›® HTMLï¼ˆå¯é€‰ï¼‰
    # spider.inspect_crawl(start_page=1, end_page=10)