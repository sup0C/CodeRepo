# import os
# import json
# import asyncio
from langchain_ollama import ChatOllama
from typing import List, Dict, Any, Optional
from datetime import datetime
# å¯¼å…¥å„ç»„ä»¶
# from hyde import build_hyde_engine, Settings
from query_decomposition import plan_query
from corrective_rag import create_grag
# from reranker import Reranker
from dynamic_prompting import LearningManager
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
# æ ¸å¿ƒä¾èµ–
# from llama_index.core import VectorStoreIndex, Document, SimpleDirectoryReader
# from llama_index.llms.openai import OpenAI
from langchain_core.prompts import PromptTemplate
# from sentence_transformers import CrossEncoder

from langchain_ollama import OllamaEmbeddings
embeddings = OllamaEmbeddings(model="znbang/bge:large-en-v1.5-q8_0")  # Qwen3-4b


class SelfHealingRAGSystem:
    """
    å®Œæ•´è‡ªæ„ˆRAGç³»ç»Ÿï¼Œæ•´åˆå…¨éƒ¨ç»„ä»¶
    """
    def __init__(self, model:str='qwen3-1.7b',openai_api_key: str = None):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
            # ç»„ä»¶åˆå§‹åŒ–
        print("ğŸš€ Initializing Self-Healing RAG System...")
        # æ ¸å¿ƒLLM - ç”¨æˆ·æ•´åˆæ‰€æœ‰èµ„æ–™è¿›è¡Œæœ€åçš„å›ç­”
        self.llm = ChatOllama(model=model, temperature=0.001)

        # åˆå§‹åŒ–å„ç»„ä»¶
        # self.reranker = Reranker()
        self.learning_manager = LearningManager()
        self.vector_index = None #
        self.hyde_engine = None #
        self.web_seaerch=0 # ç½‘ç»œæŸ¥è¯¢çš„æ¬¡æ•°
        # æ¼”ç¤ºæ•°æ®
        self.sample_documents = self._create_sample_documents()
        self._setup_vector_index()
        # self.sample_texts=''

        # ç»Ÿè®¡
        self.query_stats = {
            "total_queries": 0,
            "hyde_used": 0,
            "decomposed_queries": 0,
            "crag_activated": 0,
            "reranked": 0,
            "learning_applied": 0
        }
        print("âœ… System initialized successfully!")

    def _create_sample_documents(self) -> List[Document]:
        """åˆ›å»ºæ¼”ç¤ºç”¨çš„ç¤ºä¾‹æ–‡æ¡£"""
        sample_texts = [
            """Retrieval-Augmented Generation (RAG) is a technique that combines   
            pre-trained language models with external knowledge retrieval. RAG systems   
            retrieve relevant documents from a knowledge base and use them to generate   
            more accurate and factual responses.""",

            """Corrective RAG (CRAG) introduces a self-correction mechanism that grades   
            retrieved documents for relevance. If documents are deemed irrelevant, the   
            system triggers alternative retrieval strategies like web search.""",

            """HyDE (Hypothetical Document Embeddings) improves retrieval by generating   
            hypothetical documents that answer the query, then searching for real documents   
            similar to these hypothetical ones.""",

            """Cross-encoder reranking provides more accurate document scoring compared   
            to bi-encoder similarity search. It processes query-document pairs together   
            to produce refined relevance scores.""",

            """DSPy enables automatic prompt optimization by treating prompts as programs   
            that can be compiled and optimized against specific metrics like accuracy   
            or semantic similarity.""",

            """Self-healing RAG systems implement feedback loops that learn from successful   
            query-answer pairs, storing them as examples for future similar queries to   
            improve performance over time.""",

            """Query decomposition breaks complex multi-part questions into atomic   
            sub-queries that can be individually processed and then combined for   
            comprehensive answers.""",

            """Vector databases enable semantic search by converting documents into   
            high-dimensional embeddings that capture semantic meaning rather than   
            just keyword matches.""" ]

        return [Document(page_content=text, metadata={"id": i}) for i, text in enumerate(sample_texts)]

    def _setup_vector_index(self):
        """ç”¨ç¤ºä¾‹æ–‡æ¡£æ„å»ºå‘é‡ç´¢å¼•"""
        print("ğŸ“š Setting up vector index...")
        self.vector_index = Chroma.from_documents(self.sample_documents, embedding=embeddings)
        # self.vector_index = VectorStoreIndex.from_documents(self.sample_documents)
        # self.hyde_engine = build_hyde_engine(self.vector_index)
        print("âœ… Vector index ready!")

    def enhanced_retrieve(self, query: str, use_hyde: bool = False, top_k: int = 2) -> List[Document]:
        """æ”¯æŒHyDEçš„å¢å¼ºæ£€ç´¢
        queryï¼šç”¨æˆ·çš„æŸ¥è¯¢
        use_hydeï¼šæ˜¯å¦ä½¿ç”¨hydeæ–¹æ³•
        top_k:ç•™æœ€ç›¸ä¼¼çš„å‰Nä¸ªç­”æ¡ˆ
        """
        print(f"ğŸ” Retrieving documents for: '{query}'")
        if use_hyde:
            print(" ğŸ§  Using HyDE for enhanced retrieval...")
            response = self.hyde_engine.query(query)
            # ä»HyDEå“åº”æå–æ–‡æ¡£
            documents = response.source_nodes
            self.query_stats["hyde_used"] += 1
        else:
            print("  ğŸ“– Using standard retrieval...")
            retriever = self.vector_index.as_retriever(search_kwargs={"k": top_k, # æœ€å¤§æ£€ç´¢æ•°ï¼Œ
                  },)
            nodes = retriever.invoke(query)
            # retriever = self.vector_index.as_retriever(similarity_top_k=top_k)
            # nodes = retriever.retrieve(query)
            documents = nodes
        # è½¬æ¢ä¸ºDocumentå¯¹è±¡
        docs = []
        for node in documents:
            doc = Document(
                page_content=node.page_content if hasattr(node, 'text') else str(node),
                metadata=node.metadata if hasattr(node, 'metadata') else {})
            docs.append(doc)
        print(f"  âœ… Retrieved {len(docs)} documents")
        return docs

    def decompose_and_retrieve(self, query: str,top_k:int=3) -> tuple[List[str], List[Document]]:
        """åˆ†è§£å¤æ‚æŸ¥è¯¢å¹¶åˆ†åˆ«æ£€ç´¢
        åœ¨æœç´¢å›ç­”æ—¶ä½¿ç”¨äº†hydeå¢å¼ºæ£€ç´¢ã€‚
        input: queryï¼šç”¨æˆ·çš„åŸå§‹é—®é¢˜
        top_k:ç•™æœ€ç›¸ä¼¼çš„å‰Nä¸ªç­”æ¡ˆ
        returnï¼š[query]ï¼šList[str]ã€‚å¤æ‚åŸå§‹é—®é¢˜åˆ†è§£å‡ºçš„Nä¸ªå­é—®é¢˜ï¼Œæˆ–è€…æ— éœ€åˆ†è§£çš„åŸå§‹é—®é¢˜
                docsï¼šList[Document]ã€‚æ‰€æœ‰å­é—®é¢˜çš„å›ç­”é›†åˆï¼Œæˆ–åŸå§‹é—®é¢˜çš„å›ç­”.
                                ä¸€ä¸ªé—®é¢˜å¯èƒ½ä¼šæœ‰å¤šä¸ªå›ç­”ã€‚
        """
        print(f"ğŸ”§ Decomposing query: '{query}'")
        try:
            sub_queries = plan_query(query) # å°†å¤æ‚é—®é¢˜åˆ†è§£å‡ºçš„Nä¸ªå­é—®é¢˜
            if len(sub_queries) > 1:
                print(f" ğŸ“ Decomposed into {len(sub_queries)} sub-queries:")
                # æ‰“å°æŸ¥çœ‹æ¯ä¸ªåˆ†è§£çš„å­é—®é¢˜
                # for i, sq in enumerate(sub_queries):
                #     print(f"{i}. {sq}")

                # å¯¹æ¯ä¸ªå­æŸ¥è¯¢æ£€ç´¢
                all_docs = []
                for sq in sub_queries:
                    docs = self.enhanced_retrieve(sq, use_hyde=False, top_k=top_k)
                    all_docs.extend(docs)
                self.query_stats["decomposed_queries"] += 1
                return sub_queries, all_docs
            else:
                print("  â¡ï¸ Query doesn't need decomposition")
                docs = self.enhanced_retrieve(query)
                return [query], docs
        except Exception as e:
            print(f"  âš ï¸ Error in decomposition: {e}")
            docs = self.enhanced_retrieve(query)
            return [query], docs

    def apply_crag(self, query: str, documents: List[Document]) -> List[Document]: # tuple[List[Document], str]:
        """åº”ç”¨CRAGè¿‡æ»¤æ–‡æ¡£
        input: queryï¼šç”¨æˆ·çš„åŸå§‹é—®é¢˜æˆ–åˆ†è§£å‡ºçš„å­é—®é¢˜
            documents:é’ˆå¯¹æ¯ä¸ªå­é—®é¢˜æ£€ç´¢å‡ºçš„ç›¸å…³æ€§æ–‡æ¡£ã€‚ä¸€ä¸ªé—®é¢˜å¯èƒ½ä¼šæœ‰å¤šä¸ªå›ç­”ã€‚
        """
        print("ğŸ” Applying CRAG (Corrective RAG)...")
        try:
            # å‡†å¤‡CRAGçŠ¶æ€
            initial_state = {'question': query,
                             'generation': "",
                             'web_search': "No",
                             'documents': documents}
            app = create_grag()
            result = app.invoke(initial_state)

            if result['web_search']=='Yes':
                self.web_seaerch+=1 # è¿›è¡Œweb_seaerchçš„æ¬¡æ•°
                # å¯¹webæ•°æ®åˆ†å‰²
                # è®¡ç®—ç›¸ä¼¼åº¦é˜ˆå€¼
                # ä½¿ç”¨LLMåŸºäºè¿‡æ»¤åçš„æ–‡æ¡£è¿›è¡Œå›ç­”
                pass
            else:
                # ä½¿ç”¨LLMåŸºäºåŸå§‹æ–‡æ¡£è¿›è¡Œå›ç­”
                result['documents'] # Nä¸ªç›¸å…³æ€§æ–‡æ¡£
                pass

            # # æ­£å¸¸æƒ…å†µä¸‹ä¼šè·‘å®Œæ•´CRAGæµç¨‹
            # filtered_docs = [] # ç›¸å…³æ€§æ–‡æ¡£å­˜å‚¨å™¨
            # for doc in documents[:3]:  # æ¼”ç¤ºé™åˆ¶
            #     # ç®€å•ç›¸å…³æ€§æ£€æŸ¥ï¼ˆå®é™…åº”è¯¥ç”¨LLMï¼‰
            #     if any(keyword in doc.page_content.lower() for keyword in query.lower().split()):
            #         filtered_docs.append(doc)

            # if len(filtered_docs) < len(documents):
            #     self.query_stats["crag_activated"] += 1
            #     print(f"  ğŸš¨ CRAG filtered {len(documents) - len(filtered_docs)} irrelevant documents")

            return "Documents filtered by CRAG"

        except Exception as e:
            print(f"  âš ï¸ Error in CRAG: {e}")
            return documents, "CRAG not applied due to error"

    def apply_reranking(self, query: str, documents: List[Document], top_k: int = 3) -> List[Document]:
        """äº¤å‰ç¼–ç å™¨é‡æ’åº"""
        print("ğŸ¯ Applying cross-encoder reranking...")
        try:
            # æå–æ–‡æœ¬ç”¨äºé‡æ’åº
            doc_texts = [doc.page_content for doc in documents]

            if len(doc_texts) > 1:
                reranked_texts = self.reranker.rerank(query, doc_texts, top_k)

                # æ˜ å°„å›Documentå¯¹è±¡
                reranked_docs = []
                for text in reranked_texts:
                    for doc in documents:
                        if doc.page_content == text:
                            reranked_docs.append(doc)
                            break

                self.query_stats["reranked"] += 1
                print(f"  âœ… Reranked to top {len(reranked_docs)} documents")
                return reranked_docs
            else:
                print("  â¡ï¸ Not enough documents for reranking")
                return documents

        except Exception as e:
            print(f"  âš ï¸ Error in reranking: {e}")
            return documents

    def apply_dynamic_prompting(self, query: str) -> str:
        """
        åŠ¨æ€å°‘æ ·æœ¬å­¦ä¹ ã€‚
        queryï¼šç”¨æˆ·çš„æŸ¥è¯¢
        returnï¼šç­›é€‰å‡ºçš„Nä¸ªä¼˜ç§€é—®ç­”å¯¹æˆ–ç©ºå­—ç¬¦ä¸²ã€‚
        """
        print("ğŸ§  Applying dynamic prompting...")
        try:
            # æ·»åŠ ç§¯æé—®ç­”å¯¹ä¾‹å­
            # few_shot_context = self.learning_manager.add_positive_example(query)
            few_shot_context = self.learning_manager.get_dynamic_prompt(query)
            if few_shot_context:
                self.query_stats["learning_applied"] += 1
                print("  âœ… Applied learned examples from previous successes")
            else:
                print("  â¡ï¸ No relevant past examples found")
            return few_shot_context
        except Exception as e:
            print(f"  âš ï¸ Error in dynamic prompting: {e}")
            return ""

    def generate_answer(self, query: str, documents: List[Document], few_shot_context: str = "") -> str:
        """åŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ"""
        print("âœï¸ Generating final answer...")
        # åˆå¹¶æ–‡æ¡£å†…å®¹
        context = "\n\n".join([doc.page_content for doc in documents])
        # æ„å»ºpromptï¼Œå¯é€‰åŒ…å«å°‘æ ·æœ¬ç¤ºä¾‹
        # æç¤ºè¯æ¨¡æ¿è¿™é‡Œè¿˜éœ€è¦å†æ”¹è¿›
        # prompt_parts = []
        # if few_shot_context:
        #     prompt_parts.append(few_shot_context)
        #
        # prompt_parts.extend(["Context:",context,
        #     f"\nQuestion: {query}",
        #     "\nAnswer based on the provided context:"])
        # prompt = "\n".join(prompt_parts)
        #
        # try:
        #     response = self.llm.complete(prompt)
        #     answer = response.text.strip()
            print("  âœ… Answer generated successfully")
            return answer
        except Exception as e:
            print(f"  âš ï¸ Error generating answer: {e}")
            return f"I apologize, but I encountered an error generating an answer: {e}"

    def _get_components_used(self) -> List[str]:
        """è·å–æœ¬æ¬¡æŸ¥è¯¢ç”¨åˆ°çš„ç»„ä»¶"""
        components = ["Vector Retrieval"]

        if self.query_stats["hyde_used"] > 0:
            components.append("HyDE")
        if self.query_stats["decomposed_queries"] > 0:
            components.append("Query Decomposition")
        if self.query_stats["crag_activated"] > 0:
            components.append("CRAG")
        if self.query_stats["reranked"] > 0:
            components.append("Cross-Encoder Reranking")
        if self.query_stats["learning_applied"] > 0:
            components.append("Dynamic Prompting")
        return components

    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {"total_queries": self.query_stats["total_queries"],
            "hyde_usage_rate": f"{(self.query_stats['hyde_used'] / max(1, self.query_stats['total_queries']) * 100):.1f}%",
            "decomposition_rate": f"{(self.query_stats['decomposed_queries'] / max(1, self.query_stats['total_queries']) * 100):.1f}%",
            "crag_activation_rate": f"{(self.query_stats['crag_activated'] / max(1, self.query_stats['total_queries']) * 100):.1f}%",
            "reranking_rate": f"{(self.query_stats['reranked'] / max(1, self.query_stats['total_queries']) * 100):.1f}%",
            "learning_rate": f"{(self.query_stats['learning_applied'] / max(1, self.query_stats['total_queries']) * 100):.1f}%",
            "learned_examples": len(self.learning_manager.good_examples)}

    def full_pipeline(self, query: str, user_feedback: bool = None, previous_answer: str = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´è‡ªæ„ˆRAGç®¡é“
        input
            query:ç”¨æˆ·çš„å•æ¡æŸ¥è¯¢
            user_feedbackï¼šuser_feedback -
            previous_answerï¼š
        """
        start_time = datetime.now()
        print(f"\nğŸ”„ Starting Self-Healing RAG Pipeline")
        print("=" * 60)
        self.query_stats["total_queries"] += 1

        # æ­¥éª¤1ï¼šæŸ¥è¯¢å¢å¼º
        # å…ˆé—®é¢˜åˆ†è§£ï¼Œå†è¿›è¡ŒçŸ¥è¯†æ£€ç´¢ã€‚æ£€ç´¢æ—¶ï¼Œè‹¥èƒ½hydeåˆ™hydeï¼Œå¦åˆ™æ­£å¸¸å‘é‡æ£€ç´¢
        sub_queries, documents = self.decompose_and_retrieve(query)
        # æ­¥éª¤2ï¼šæ–‡æ¡£æ ¡éªŒï¼ˆCRAGï¼‰
        filtered_docs = self.apply_crag(query, documents)
        # æ­¥éª¤3ï¼šæ–‡æ¡£é‡æ’åº
        # reranked_docs = self.apply_reranking(query, filtered_docs)
        # æ­¥éª¤4ï¼šåŠ¨æ€æç¤º - è‡ªè¿›åŒ–ã€è‡ªæ¼”åŒ–ã€‚
        # å°†æ¯ä¸ªé—®ç­”å¯¹éƒ½å­˜å…¥ç§¯ææ¡ˆä¾‹åº“ä¸­ï¼Œé€æ¸å¢å¼ºç³»ç»Ÿçš„é²æ£’æ€§å’Œç¨³å®šæ€§
        # few_shot_context = self.apply_dynamic_prompting(query)
        # æ­¥éª¤5ï¼šç­”æ¡ˆç”Ÿæˆ -
        answer = self.generate_answer(query, filtered_docs)
        # æ­¥éª¤6ï¼šå­¦ä¹ ï¼ˆå¦‚æœ‰åé¦ˆï¼‰ - è‡ªæ›´æ–°åŠ¨æ€æç¤º
        # å¯ä»¥è€ƒè™‘å°†è¿™ä¸ªç§¯ææ¡ˆä¾‹åº“ç‹¬ç«‹å‡ºæ¥ï¼Œç„¶åæ ¹æ®å‡†ç¡®ç‡æŒ‡æ ‡åˆ¤æ–­ï¼Œå¦‚æœå¤§äºæ¯ä¸ªé˜ˆå€¼åˆ™è®¤ä¸ºä¸ºå¥½çš„é—®ç­”å¯¹ã€‚
        # if user_feedback is True:
        #     try:
        #         self.learning_manager.add_good_example(query, answer)
        #         print("ğŸ“š Added successful example to learning system")
        #     except Exception as e:
        #         print(f"âš ï¸ Error adding to learning system: {e}")
        end_time = datetime.now()
        # å•æ¡é—®é¢˜çš„è¿è¡Œæ—¶é—´
        processing_time = (end_time - start_time).total_seconds()
        result = {
            "query": query,
            "sub_queries": sub_queries,
            "documents_found": len(documents),
            "documents_filtered": len(filtered_docs),
            # "final_documents": len(reranked_docs),
            "answer": answer,
            # "crag_status": crag_status,
            "processing_time": processing_time,
            "components_used": self._get_components_used()}
        print(f"âœ… Pipeline completed in {processing_time:.2f} seconds")
        # print(f"ğŸ“Š Documents: {len(documents)} â†’ {len(filtered_docs)} â†’ {len(reranked_docs)}")
        return result

def demo_interactive_session(demo_queries:List[str]=['']):
    """äº¤äº’å¼æ¼”ç¤º
    demo_queriesï¼šç”¨æˆ·çš„æé—®ã€‚
    """
    print("""ğŸ¯ Self-Healing RAG System Demo  
    ================================  
    This system demonstrates:
    â€¢ HyDE: Hypothetical Document Embeddings  
    â€¢ Query Decomposition: Breaking complex queries  
    â€¢ CRAG: Corrective RAG with document grading  
    â€¢ Cross-Encoder Reranking: Precision ranking  
    â€¢ Dynamic Learning: Few-shot from success examples""")
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = SelfHealingRAGSystem()
    # æ¼”ç¤ºç”¨æŸ¥è¯¢
    print("ğŸ”¥ Running Demo Queries...\n","=" * 50)

    results = []
    # å¯¹æ¯ä¸ªé—®é¢˜ä¾æ¬¡å¾ªç¯å¤„ç†
    for i, query in enumerate(demo_queries):
        print(f"ğŸ“‹ Demo Query {i}/{len(demo_queries)}\n",
              f"Query: '{query}'")
        # æ¨¡æ‹Ÿæ­£åé¦ˆç”¨äºå­¦ä¹ 
        if i > 1:  # ç¬¬äºŒä¸ªæŸ¥è¯¢å¼€å§‹åŠ åé¦ˆ
            result=system.full_pipeline(query, user_feedback=True)
        else:
            result = system.full_pipeline(query) # å¼€å§‹è¿è¡Œè‡ªæ„ˆRAGç³»ç»Ÿ
        results.append(result) # å°†å½“å‰é—®é¢˜çš„ç»“æœä¿å­˜
        print(f"ğŸ’¡ Answer:",f"{result['answer']}")
        print(f"\nğŸ“Š Components Used: {', '.join(result['components_used'])}")

    # æœ€ç»ˆç»Ÿè®¡
    print("=" * 60,"\nğŸ“ˆ SYSTEM PERFORMANCE STATISTICS\n","=" * 60)
    stats = system.get_system_stats() # è·å–ç³»ç»Ÿæ‰§è¡Œçš„ç»Ÿè®¡æ•°æ®
    for key, value in stats.items():
        print(f"{key.replace('_', ' ').title()}: {value}")

    return system, results


if __name__ == "__main__":
    demo_queries=["What is RAG and how does it work?",
        "Compare HyDE and standard retrieval methods",
        "How does CRAG improve retrieval quality and what are the benefits of cross-encoder reranking?",
        "Explain the self-correction mechanisms in modern RAG systems",
        "What are the advantages of DSPy optimization for prompts?"]
    # è®¾ç½®OpenAI APIå¯†é’¥
    # os.environ["OPENAI_API_KEY"] = "your-key-here"
    demo_interactive_session(demo_queries)